{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename)) # prints the full path of each file\nimport tensorflow as tf\nimport tensorflow_decision_forests as tfdf","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-24T16:10:14.466504Z","iopub.execute_input":"2024-11-24T16:10:14.467023Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"**Load DataSet**","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\nserving_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\ntrain_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Prepare dataset:\n#Tokenize the name\n#\"Braund, Mr. Owen Harris\" will become [\"Braund\", \"Mr.\", \"Owen\", \"Harris\"]\n#Extract any prefix ticket\n#\"STON/O2. 3101282\" will become \"STON/O2. and 310282","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#function that takes in a dataframe df as input\ndef preprocess(df):\n\n    #take copy of data\n    df = df.copy()\n\n    #strip special characters, join in single string w spaces\n    def normalize_name(x):\n        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n\n    #extracts last part of ticket\n    def ticket_number(x):\n        return x.split(\" \")[-1]\n    #extracts first part of ticket, in not then 'none'\n    def ticket_item(x):\n        items = x.split(\" \")\n        if len(items) == 1:\n            return \"NONE\"\n        return \"_\".join(items[0:-1])\n\n    #new dataframe\n    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)                     \n    return df\n\n#run preproccess function above on training and testing functions\npreprocessed_train_df = preprocess(train_df)\npreprocessed_serving_df = preprocess(serving_df)\n\npreprocessed_train_df.head(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#remove some features: we dont want train model on passenegrID,and ticket\ninput_features = list(preprocessed_train_df.columns)\ninput_features.remove(\"Ticket\")\ninput_features.remove(\"PassengerId\")\ninput_features.remove(\"Survived\")\n#input_features.remove(\"Ticket_number\")\n\nprint(f\"Input features: {input_features}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize_names(features, labels=None):\n    #Divde the names into individual tokens\n    features[\"Name\"] =  tf.strings.split(features[\"Name\"])\n    return features, labels\n\n#tfdf.keras.pd_dataframe_to_tf_dataset : converts pandas df to tf df\n#label = survived : Specifies the label column for supervised learning\n#Applies the tokenize_names function to each row in the dataset\ntrain_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df,label=\"Survived\").map(tokenize_names)\ntest_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#First training a GradientBoostedTreesModel model with the default parameterst\n\n\n#initialize model\n\n#keras.GradientBoostedTreesModel: Ensemble learning algorithm combining decision trees\n#works by iteratively building trees to minimize the error of previous trees\nmodel = tfdf.keras.GradientBoostedTreesModel(\n    \n    #Reduces amount of logging informationfor simplicity\n    verbose=0,\n    \n    #Specifies the features to use for training\n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    #other features ignored\n    exclude_non_specified_features=True,\n    #Sets a fixed random seed for reproducibility\n    random_seed=1234,\n)\n\n#Trains the Gradient Boosted Trees model on the training dataset \nmodel.fit(train_ds)\n\n#Evaluates the model's performance using the training dataset and prints\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Train model with improved default parameters\n\nmodel = tfdf.keras.GradientBoostedTreesModel(\n    verbose=0, \n    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n    exclude_non_specified_features=True,\n    \n    #Enables computation of permutation-based feature importance\n    #slower but provides a deep understanding of the feature contributions\n    #compute_permutation_variable_importance=True,\n\n    # Uncommenting this could override the manual parameters\n    # Change the default hyper-parameters\n    # hyperparameter_template=\"benchmark_rank1@v1\",\n    \n    #num_trees=1000,\n    #tuner=tuner\n\n    # Sets the min number of examples node for further splitting. Lower values more granular splits\n    min_examples=1,\n\n    #selects splits based on randomized subsets of categories\n    categorical_algorithm=\"RANDOM\",\n\n    #Limits the depth of the trees to control overfitting\n    #max_depth=4,\n    \n    #aka learning rate: contribution of each tree to final prediction. Smaller = more accurate, slower training\n    shrinkage=0.05,\n\n    #proportion of features considered for each split, reduces training time\n    #num_candidate_attributes_ratio=0.2,\n\n    #creates splits that involve linear combinations of features. To handle high-dimensional data effectively\n    split_axis=\"SPARSE_OBLIQUE\",\n    \n    #normalize data using min_max metho data before splitting\n    sparse_oblique_normalization=\"MIN_MAX\",\n    \n    #higher val = complex splits,capture more subtle patterns\n    sparse_oblique_num_projections_exponent=2.0,\n\n    #number of trees in ensemble, incr accuracy\n    num_trees=2000,\n\n    #ratio of training data reserved for validation\n    #validation_ratio=0.0,\n    \n    random_seed=1234,\n    )\n\n#train model\nmodel.fit(train_ds)\n\nself_evaluation = model.make_inspector().evaluation()\nprint(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Predictions\ndef prediction_format(model, threshold=0.5):\n    pb_survive = model.predict(test_ds, verbose=0)[:,0]\n    return pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        #cutoff probability = threshold = 0.5\n        \"Survived\": (pb_survive >= threshold).astype(int)\n    })\n    \ndef make_submission(predictions):\n    path=\"/kaggle/working/submission.csv\"\n    predictions.to_csv(path, index=False)\n    print(f\"Submission exported to {path}\")\n\npredictions = prediction_format(model)\nmake_submission(predictions)\n!head /kaggle/working/submission.csv","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}